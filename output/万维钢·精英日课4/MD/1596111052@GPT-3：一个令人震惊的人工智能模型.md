# GPT3：一个令人震惊的人工智能模型

如果你还不知道的话，现在你应该知道。七月份爆出一个大新闻，这也可能是 2020 年这个奇异的年份的七月里最重要的新闻，人工智能科研公司 OpenAI，推出了它的新一代语言模型，叫 GPT-3。这个事儿有多厉害呢，这几天中文社交网络流传一篇文章的标题叫《一个新型 AI 震惊硅谷：它像一个高智商的人，颠覆对 AI 的认知》，这个标题并不算太夸张。你可以去读一下那篇文章。

GPT-3 不只是一个理论也不是一个成熟的产品，而是一个存在于云上、已经训练好了随时可以用的人工智能“模型”。如果你是一个程序员，现在可以向 OpenAI 申请参加内测。据说它将在今年之内实现商业运营。使用 GPT-3，你面对的只是一个简单的文字输入输出接口。但是接口的背后，是一个无比深邃又无比广博的汪洋大海。

它能干什么呢？ **GPT-3 是一个语言模型。它不下围棋、不帮你自动驾驶也不搞图像识别，它做的都是能用语言描述的事情。**

而能用语言描述的事儿实在太多了。

*

可惜我没用过。但是这几天 Twitter 上有无数程序员贴出了他们的使用发现，每次都伴随着赞叹。比如说，编程语言也是语言，对吧？GPT-3 不但会编程，而且能根据你用人类语言提供的要求，替你编写代码。下面这段演示来自 Twitter —— 

![https://piccdn3.umiwi.com/img/202007/30/202007302011509922263511.gif](https://piccdn3.umiwi.com/img/202007/30/202007302011509922263511.gif)

比如你输入“彩虹的每个颜色的按钮”，GPT-3 就会生成一段网页代码，这段代码的效果是生成七个按钮，每个按钮对应彩虹上的一个颜色。你输入“世界上最富有国家的名字和 GDP 列表”，GPT-3 就给输出一段能生成这个表格的代码，然后点击查看效果。你不必告诉它任何细节，它自己补充。

GPT-3 不但能写代码，而且能给你写一个完整的、尽管是简单的、应用程序。你用人类的语言告诉它你想要几个按钮，都有什么功能，它就给你做出来。

传统的文字处理就更能了。GPT-3 可以根据很少的输入信息自行“创作”一个东西。你给一个标题和第一句话，它就能哗哗哗生成一篇文章。你输入一个单词，它能给你写一条 tweet。

关键它不是胡乱写，前言后语似乎有一定的逻辑，内容还能切合题意。比如你让它以 Facebook CEO 的名字“扎克伯格”为题写一条 tweet，它写的是“离扎克伯格远点，现在最危险的事儿就是科技公司正在进军金融领域。”有人在写一篇叫做《如何高效地开董事会》的文章，自己写了一半，把这个前半部分输入到 GPT-3, GPT-3 就把后半部分给写出来了，有理有据，还列出了原作者未曾想到的几个点。它可以用你的语言风格写这篇文章。你要是指定一个作家的名字，它还能使用那个作家的语言风格。

GPT-3 还会写诗、写情书、写产品说明书、写剧本。它还会像个真人一样跟人对话，它会翻译，会回答问题，会完形填空，会做阅读理解题，而且会做 SAT 考试题。

程序员们的反应速度也真是非常快，短短几天，人们就开发了好几个基于 GPT-3 的应用。

那么 GPT-3 这个水平，到底有高级呢？它的原理是什么？它有多大实用价值？它对世界意味着什么呢？

*

给个标题写篇文章，这个其实谁都会，但是魔鬼都在细节之中。你这篇文章的内容有没有可读性，逻辑是否连贯，信息是否新颖，思想有没有创造性，这才是好坏的关键。 **GPT-3 跟当前所有主流人工智能模型一样，都是基于深度学习神经网络的东西，它仍然是海量数据训练出来的产物，它提供的仍然是基于经验的知识……甚至不能说是知识，只能说是反应。**

 *但是量变有时候就是质变，“大”有时候就是一种创新。* GPT-3 是现在规模最大的神经网络。它有多大呢，那必须是一个“大写的大”。

衡量一个神经网络的大小可以看它有多少个“参数”。网络的每一个神经元结构点上都有若干个可调参数，参数描写了这个网络。我们专栏以前讲过一个能识别手写阿拉伯数字的简单神经网络 [1]，它有  *1 万* 多个参数。2012 年在 ImageNet 竞赛中获得冠军，达到实用程度，代表当时图形识别技术最先进水平的 AlexNet，有 *6200 万* 个参数 [2]。参数越多，神经网络就越复杂，会的东西就越多，但是参数多意味着计算量大，而因为语言比图形复杂得多，语言模型需要更多的参数。

硅谷创业圈有个概念叫“十倍思维”，说你这个新产品得比别人好十倍，拿出来才有意思，我感觉现在这些公司弄的神经网络就很有十倍思维。GPT-3 的上一代，OpenAI 的 GPT-2，有 *15 亿* 个参数。Nvidia 公司的一个语言模型 Megatron，有 *80 亿* 个参数。微软的“图灵 NLG”，有 *170 亿* 个参数。而GPT-3，有 **1750 亿** 个参数。

 **1750 亿。** 请问这是什么样的运算量。网络大还不算，喂给 GPT-3 的文本数据，有 45TB。你基本上可以说 GPT-3 阅读了网上能找到的所有书籍、文章、文档、计算机代码手册等等各种算得上是“语言”的东西。

所以想要在现代世界取得一点突出的成绩真是得走极端啊。有这么大的规模，意味着 GPT-3 可以做通用的事情。你像 AlphaGo 受到的训练就是下围棋，你让玩个别的就得重新练 —— 而 GPT-3，因为它什么文本都读，它可以什么事儿都做 [3,4]。以前 GPT-2 还需要针对特殊任务做专门的训练，而 GPT-3 完全不需要。

你要做的就是把这 1750 亿个参数搭建好，然后把各种文本源源不断地喂给它，它就自己去学习了。你甚至不需要对那些训练素材进行人工标记。

*

2010 年代以后的语言模型全都是基于深度神经网络（DNN）的，这是最先进的思想，但根本的原理仍然是对经验的识别。语言模型是干什么的呢？它的本质是判断一个句子在真实世界中存在的可能性 [5]。比如下面这两句话 ——

1. 我带着狗出去散步。

2. 我带着香蕉出去散步。

语言模型知道第一句的可能性更大。这个知识不用特意教，模型并不知道狗和香蕉都是什么东西，散步是什么意思：它只是阅读了太多的文字，它知道当人们说“散步”的时候更容易提到“狗”，而不是“香蕉”。

 *所有的能力，都是基于这一个简单的道理。但是你可能想不到，把这个道理用到极致，意味着什么。*

扎克伯格的名字总是跟“技术”联系在一起，而 Facebook 最近的确在搞互联网“金融”，Facebook 现在的公众形象的确是让人产生防备意识。GPT-3 “知道”这一切，所以它拿“扎克伯格”造的句子才显得那么合理。

除了大之外，GPT-3 还是用了两个新技术 [3]。我也不太懂，只能大概说说。

一个是对词汇的“矢量表示（word vector representations）”。我理解现在模型眼中词与词之间的关系变得更深了，以至于你可以做一些逻辑上的加减法运算。比如说

巴黎 - 法国 + 意大利 = ？

你猜等于什么？罗马。巴黎跟法国的最常见关系是法国的首都，巴黎 - 法国，大约就是去除法国元素但是保留首都元素；再加上意大利，那就是首都加意大利，也就是意大利首都罗马。类似的例子还有

科学家 - 爱因斯坦 + 毕加索 = 画家

Windows - 微软 + Google = 安卓

你自己体会。另一个新技术是 2017 年“Google 大脑”和多伦多大学搞出来的所谓“Transformer 架构”。这个架构允许神经网络搭建更深的分层，这样可以阅读非常长的句子，并且确保能摘取到其中的关键点。我理解这就如同能从长句中抓住中心思想一样，不至于读着读着把意思混淆了。

这些理论非常新，但都是公开发表的论文，是来自其他科学家的贡献。GPT-3 只是用这么大规模的模型把这些思想实现了。

*

但大了确实不一样，GPT-3 的一个超能力是你只要给他非常少的几个例子，它就明白你想让它做什么。比如你先让它看一道填空题 ——

爱丽丝和鲍勃是朋友。爱丽丝去看他的朋友 ____

并且告诉它答案是“鲍勃”。这就行了。然后你再给它一道题 ——

乔治买了一些棒球装备，有球，手套，和 ____

GPT-3 会猜到这回的答案是球棒。只用一个例子。Twitter 上已经有人发现，你给几个把法语翻译成英语的例子，GPT-3 就会接着帮你把法语翻译成英语。你给几个把法律条文用老百姓的语言叙述一遍的例子，GPT-3 也会接着做，而且还会反过来，把老百姓语言翻译成法律条文。

这不是太神了吗？只用少数几个例子就能学会，这难道不是人脑特有的能力 [6] 吗？

机器学习界把这个能力叫做“few-shot learning”，但是请注意，GPT-3 并不是真的能用那么少的例子学习。我理解你给的那几个例子，其实作用仅仅是让 GPT-3 明白你要做什么，而不是学会怎样做。怎样做，那是它早就用海量的数据训练好了的能力。

所以我们千万不要过分乐观。连 OpenAI 的 CEO 都说，GPT-3 这个热度其实是个 hype。

*

GPT-3 写出来的文章能给你惊喜，但是也会让你恼火。测试表明它说的很多话仍然有前言不搭后语的现象，特别是它有时候会说一些让人感到冒犯的话，比如说歧视女性。它只是在根据经验做事，它并不真的理解自己在说什么。

但是这已经很好了。我希望 GPT-3 能取代一部分 Google 的事儿。我们想要调研一个什么新闻话题，现在的做法是在搜索引擎一篇一篇地查看文章，也许 GPT-3 可以自动为我们生成一篇报道。它可以做一个信息搜集助手。

很多程序员需要一段代码的时候不是自己写，而是上网搜索一段现成的。现在 GPT-3 可以直接帮他们生成这段代码：本质上来说这跟自己上网搜索没区别，但是的确方便了很多。

GPT-3 的作用，也许可以这么理解。比如你是一个新员工，第一天到公司上班。你接到一个任务，刚一上手，旁边一个老员工就说话了：“你新来的吧？我们老手都是这么做的……”他给你演示了一遍。你一看这挺好，就学着他做。

这一天下来你做的几乎每件事，中午到食堂吃饭、下午开会发言、晚上下班找班车，旁边都有个老员工对你说：“你新来的吧？我们老手都是这么做的……”

GPT-3 就是一位老员工。他知道所有的“最可能做法”，这对你很有用。但是你的价值，是有一天对那些老员工说：“你们是老员工吧？现在我建议咱们换个做法，咱们应该这么做……”

*

最后，OpenAI 的投资人包括……你猜如果让 GPT-3 来写这句话，这个投资人应该是谁呢？肯定是马斯克，因为根据经验，这些最厉害的新科技背后都有马斯克的影子。你要听 GPT-3 的，你得到的就是这种“最可能”但是不一定对的答案。

……不过 OpenAI 的投资人的确包括马斯克。

## 划重点

GPT-3 跟当前所有主流人工智能模型一样，都是基于深度学习神经网络的东西，它仍然是海量数据训练出来的产物，它提供的仍然是基于经验的知识……甚至不能说是知识，只能说是反应。

但是量变有时候就是质变，“大”有时候就是一种创新。

![https://piccdn3.umiwi.com/img/202007/30/202007302013460719764766.jpg](https://piccdn3.umiwi.com/img/202007/30/202007302013460719764766.jpg)

---

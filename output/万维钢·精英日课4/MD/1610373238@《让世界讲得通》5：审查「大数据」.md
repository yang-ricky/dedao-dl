# 《让世界讲得通》5：审查「大数据」

2010 年，我曾经给《新知客》杂志写过一篇讲按现在说法叫「大数据」的文章，当时可能还没有「大数据」这个名词。我那篇文章叫《数字如潮人如水》，我说很多公司正在利用个人的数据判断消费心理。我说到一个例子是赌场根据一个老太太的输赢记录算出来了她的疼痛点，在她输太多的时候请她吃了牛排，避免她痛定思痛从此戒赌。

那篇文章引起了强烈的兴趣，还被中央电视台的一个读报节目播报了，我收获了几分钟的名望。那时候人们谈论「大数据」都是赞叹。当然我们也会想到这样的做法会不会伤害消费者利益，但是总体上，我们认为这很厉害。

现在十年过去，我们对「大数据」态度已经全变了。

有句话叫「如果你手里有一把锤子，你会看什么东西都是钉子」。以前我们谈论大数据都是想象自己是那个拿着锤子的人，我们谈论这把锤子好用不好用。

而现在，每个人已经切身领教了大数据，我们才意识到，其实我们是钉子。

*

只要你能冷静思考，什么大数据、什么算法，不但一点都不神奇，而且可以说相当笨拙。

有一个你想必早听说过的故事是这样的。美国有个父亲发现 Target 超市给他女儿寄来一些孕妇用品的减价券，可是他女儿还在上高中啊！他非常生气，就打电话向超市抱怨，超市赶紧表示了道歉。结果不久之后，他发现女儿原来真的怀孕了。

这个故事是 2012 年出来的 [1]，当时传为美谈，人都说大数据太神奇了，比你身边的人更了解你。可是你仔细想想，这一点都不神奇。那个女儿在超市的购物记录里有一项是加了叶酸的维生素补充剂，而这个东西恰恰是国家卫生局建议孕妇服用的。如果这个父亲亲眼看见女儿在服用这个药，而且他了解这个知识，他难道猜不出来女儿可能怀孕了吗？

最大的可能性是超市那个派发减价券的算法根本不需要理解“怀孕”这个事儿，它只是考虑了各种商品的关联习惯而已。而且那个父亲提出的是个真问题：这些算法的匹配准确度并不高。超市既不可能、也不需要按照每个人的精确需求派发减价券，算法特意有一定的随机性。超市经理在电话里道歉并不仅仅是出于礼貌，他完全知道算法不准。

可如果不是推荐商品，而是选拔人才呢？如果是考核呢？如果是决定犯罪嫌疑人是否能得到保释呢？算法的准确度可就是个大问题了。

我们继续说蒂姆·哈福德的《让世界讲得通》， *这一讲的主题是大数据算法的两个问题，一个是数据采样不准，一个是算法不透明。*

*

最近有一部根据同名网络小说改变的电视剧叫《赘婿》，还没播出就成了微博的热点，不过可能不是主创人员想要的热点。

人们指责男主人公，明明是给人家当赘婿出身，靠着女方的家产做大，结果还找了那么多妻妾，实在是太不尊重女性了 [2]。小说作者愤怒的香蕉的一番话更是激怒了女观众：「七年前我写《赘婿》的时候根本没考虑女读者啊，……本来就是男频爽文……为什么有女观众觉得剧需要她们？」

这句话生动地说明了大数据算法的采样偏差。如果读小说的都是男性，看电视剧的都是女性，你怎么办。这个现象绝非特例，而且绝非只在中国。

美国警用防弹背心的设计没考虑到女警察有乳房。苹果健康应用没考虑到女性生理周期。绝大多数药物的临床测试都有意避开了孕妇和哺乳期女性。就连美国刚刚批准的两个 mRNA 新冠疫苗，用了好几万人做临床三期实验，其中也没有孕妇。如果你的数据采样有系统性的偏差，数据再多也不可能准确。

而默默的忽略就等于默默的歧视。2014 年，亚马逊公司决定用大数据算法挑选应聘者的简历。这个思路听起来没毛病：我先看看我现有的优秀员工当初的简历都是什么样的，我让算法自动从这些简历中学习规律，然后按照这个规律去筛选新的简历。一切都很公正，是吧？

结果女性应聘者吃了大亏。当时亚马逊的优秀员工中，绝大多数都是男性。算法观察到了这一点，而且自动强化了这一点。如果你是一个女应聘者，哪怕你擅长一些传统上男性擅长的活动，比如你曾经入选过足球国家青年队，或者你曾经是国际象棋俱乐部的队长 —— 但是只要你的简历上写的是你是*女*足队员、*女子*国际象棋俱乐部队长，你也会被算法降级。

亚马逊一直到 2018 年才停止使用这个算法，而我们不知道有多少女性因为这个算法失去了机会。

算法想不到的地方，人可以想到，而且我们可以做好。上世纪七十年代，英国的儿童福利是以给家庭收入退税的方式发放的，而既然家庭主要收入通常来自男性，这笔钱往往就没有落到女性手上。后来英国政府意识到这是一个默默的歧视，于是改变做法，把儿童福利直接发给女性 —— 结果有经济学家注意到，女性和儿童服装的销量立即上升，男性服装的销量立即下降了。

为什么针对选举的民意测验经常不准？为什么商品销售的预测经常不对？问题往往不是出在数据量太少，而是出在数据采样的偏差。有的人不愿意接电话，不愿意填表告诉你他喜欢什么，有的人根本说不清自己喜欢什么，有的人说的跟他做的正好相反，有时候读小说的和看电视剧的是两种人。在大数据之外还有一个“暗数据” —— 那些你测不到的人的数据。你测不到他们，你的算法对他们就没有代表性，可是他们却能影响大局。

*

现在算法的另一个问题，也可能是更有“体制”味道的问题，是不透明。算法有毛病很正常，我们完全可以把它拿过来审视一番，找到毛病，提出意见。

华盛顿市使用一个叫做 IMPACT 的算法评估公立学校系统的教师表现，而且搞得非常强硬：如果算法认为哪个教师的表现不合格，她真的就会被解雇。结果是这个算法冤枉了很多好教师。

算法的逻辑很简单。美国学校是一个老师只教一个年级，也就是说她每次都是跟一个班的学生一年。算法考察这个班全体学生在上一个学年结束时候的成绩，再看看他们在本学年结束时候的成绩，两相比较，如果学生们的相对水平没进步反而还退步了，那就是这个老师没教好。你能不能想一想，这个算法可能会出什么问题。

2011 年，华盛顿市的学校系统用这个算法解雇了 206 个教师。这是非常粗暴的决定。

一个问题是学生表现有很大的不确定性。一个班总共才有不超过 30 个学生，这个样本量是非常小的。只要有那么几个学生在之前的考试中超常发挥、有几个学生在之后的考试中发挥失常，老师的前途就完了。学生可能因为家庭原因、因为校外原因，或者纯粹就是偶然地有各种发挥，这是老师控制不了的。

更大的问题在于老师可以作弊。如果六年级老师使用作弊方法大幅度提高了学生的成绩，七年级老师就等于接手了一个名义上的“天才班”，那她怎么办呢？

像这样的事儿，你必须充分了解那个算法，才可能给老师鸣冤。然而现在各个公司对招聘也好、广告推送也好、股票交易也好，各种算法都是保密的。以前如果你应聘落选了，也许还可以打电话问问对方为啥不要你；现在可能是连负责招聘的领导都想不到，你是因为简历里提到自己曾经是女足运动员而落选。

*

此时此刻，大型互联网公司正在中国遭受前所未有的猛烈质疑。人们不再赞叹他们提供的方便和平台，而是指责他们利用自己的垄断地位搞「大数据杀熟」、搞算法引流、搞有针对性的虚假广告。公众正在变得不再信任算法。

这些公司应该怎么做，才能赢回信任呢？

也许唯一的出路是把算法公开，或者最起码也要大大提高那些算法的透明度，让公众审查。

英国国家学术院院长，也是上议院议员，也是一位哲学家，奥诺拉·奥尼尔（Onora O’Neill），提出一个概念叫 *“智能开放（intelligently open）”* 。她认为智能开放的算法决策应该满足下面这四条要求 ——

1. 其中的信息是可获得的（accessible），不能藏在暗处；

2. 决策过程是可理解的（understandable）；

3. 信息是可用的（usable），比如说格式得统一；

4. 决策结果是可评估的（assessable），也就是允许别人事后评估这个算法有没有做出不公正的决定。

其实这四点是可以做到的，而且做到了也并不会伤害公司的竞争力。我们专栏以前讲过一个案例 [3]，美国 NorthPointe 公司弄了个预测犯人被释放后再犯罪率的算法，叫COMPAS。因为这个算法预测黑人的再犯罪率普遍比白人高，有人指责其中有种族歧视。NorthPointe 公司并没有公开这个算法，但是大致让人了解了算法的逻辑过程，而且提供了非常详尽的训练算法和算法预测结果的数据。

结果很多专家主动研究了那些数据，他们不但证明了那个算法没有歧视黑人 —— 因为算法根本就没考虑犯罪分子的种族 —— 而且还从统计学上证明，不管是什么算法，要想准确预测再犯罪率，黑人就一定会吃亏。

我看那个公司是这次公开评议的受益者。其实你就算把算法全部公开，竞争对手也不太可能拿去就用，因为你的算法肯定是为你这个公司的特殊业务定制的。而且你不用完全公开，只要能做到“智能开放”，把算法放到阳光下让人审查，就可以赢得信任。

 *统计学的第六个教训是问题：采样可能会有偏差；第七个教训是解决方案：算法应该开放。当你拿把锤子睨视天下的时候，别忘了钉子可能会反抗。*

![https://piccdn3.umiwi.com/img/202101/11/202101112155375187683381.gif](https://piccdn3.umiwi.com/img/202101/11/202101112155375187683381.gif)

## 划重点

统计学的第六个教训是问题：采样可能会有偏差；第七个教训是解决方案：算法应该开放。当你拿把锤子睨视天下的时候，别忘了钉子可能会反抗。

![https://piccdn3.umiwi.com/img/202101/11/202101112156168525841661.jpg](https://piccdn3.umiwi.com/img/202101/11/202101112156168525841661.jpg)

---

# 《人工不智能》4:一道烧脑的“歧视”题

今天咱们继续说布鲁萨德的《人工不智能》。我们要讲一个小道理和一个大道理。

大道理很容易理解，但是这个小道理比较烧脑，相当于是一道数学题，而且是2016年才被人想明白，还专门发表了学术论文。你并不需要任何高级数学知识，我会帮你从直观上理解这个道理。我建议你把这道题当做一个头脑体操，把玩一下其中的乐趣。

很少有什么数学思想跟实际生活有这么独特的关系。这个事儿咱们得从美国的司法制度说起。

## 1.法官用的算法

中国的犯人，理论上来说，不管是什么人，同样的罪行就意味着同样的刑期。在监狱里如果表现好，同样的表现等于同样的减刑。惩罚力度，跟你是个什么人，没关系。

但是在美国，法官判决的时候，会看犯人“是个什么人”。同样的罪行，如果是惯犯，判得就会比较重；而如果这个人平时表现良好，就可能得到轻判。

这个指导思想是看犯人将来再次犯罪的可能性大小。如果法官认为这个人不会再次犯罪，他就会给轻判，而且很容易批准假释。而如果法官判断这个人将来很可能再次犯罪，那就会从重从严判罚，假释减刑什么的也别想了，就在监狱里待着省得出去危害社会。

那怎么知道一个人是什么人呢？以前都是靠法官的主观判断，肯定是不太靠谱。所以现在是用算法来判断。有个私人公司叫 NorthPointe，开发了一个算法，叫 COMPAS，专门用来判断一个犯人再次犯罪的可能性大小。

COMPAS 算法会考察一个犯人的100多项指标，然后给他打一个分。从1分到10分，分数越高，代表将来再犯罪的概率就越高。

这就比法官个人的判断强多了，毕竟哪个法官也不可能同时考虑这么多项指标。算法是冰冷的，但算法也是客观冷静的， **对吧？**

这个算法的原理跟我们之前说的那个用人工智能判断泰坦尼克号乘客存活的项目是一样的。都是用大数据，根据以往的经验，你考察若干个关键指标，对一件事作出判断。训练算法用的都是美国的数据，犯人都是美国的犯人，美国的犯罪环境变化也不大，这一套完全合理， **对吧？**

而且这个 NorthPointe 公司还非常痛快，把自己用的所有数据都在网上公布了，你可以随便研究。它具体的算法是保密的，但是如果你不服，你可以自己开发一套算法跟它比。研究者发现 COMPAS 算法的准确率还真可以 —— 算法给打了1分的犯人，再次犯罪率只有22%；而打10分的犯人再次犯罪率高达81%。所以这个分数也还算科学， **对吧？**

一般这种犯罪率判断，最怕被人指责种族歧视。众所周知黑人的犯罪率比较高，那有没有可能，仅仅因为犯人是黑人，你就调高他的分数呢？COMPAS 算法在这一点上做的很绝 —— 它根本就不考虑犯人是不是黑人。一百多项指标包括性别、年龄、以前的犯罪历史等等，其中根本就没有种族这个项目！事实上人们统计发现，算法给打了7分的犯人中，如果是白人，后来的再次犯罪率是60%；而7分黑人的再次犯罪率是61%，几乎相等。所以这个算法真的没有对黑人的种族歧视， **对吧？**

 **对，对，对，对。** 但还是有人对 NorthPointe 公司提出了种族歧视的指控。

## 2.一道数学题

这就引出了我们要说的数学题。布鲁萨德对这件事写的非常简略，我不得不查阅了好几个文献才搞清楚。

有个专门为了社会责任搞深入新闻调查的非盈利媒体，叫 ProPublica，研究了一下 COMPAS 算法。我们知道算法预测的都是概率而已，哪怕是被打了5分以上的分数，犯人也不一定会再次犯罪。如果一个犯人明明没有后来再犯罪，却被算法打了个高分，那他就等于被算法冤枉了。

ProPublic 专门统计这种没有再犯罪，却被打了5分以上的情况。同样是后来没再犯罪的犯人，黑人被打高分的可能性是42%，而白人只有22%。那这不就是说，算法还是歧视了黑人吗？

这个发现引发了轩然大波 [1]。包括哈佛大学、斯坦福大学、康奈尔大学、卡内基梅隆大学、芝加哥大学和 Google 公司的一流专家，纷纷就此展开了研究。反正 COMPAS 的数据都是公开的，专家们可以独自验证。

说到这儿我先帮你捋一下 ——

* COMPAS 算法并不考虑犯人的种族。

* 同样的预测分数下，黑人和白人的*实际*再犯罪率是一样高的。

* 但是没有再犯罪的犯人中，黑人被算法打高分的可能性比白人大得多。

这怎么可能呢？这个算法到底是公平还是不公平呢？

## 3.这里请烧脑

这个问题非常有意义，以至于有人在2016年专门写了一篇论文 [2] 把它讲清楚。我是看了斯坦福大学的几个研究者在华盛顿邮报上的一篇博客 [3]，才算搞明白。

我们必须得看图说话。下面这张图来自华盛顿邮报，表现了黑人和白人的打分和实际的再犯罪率。

![https://piccdn3.umiwi.com/img/201805/28/201805280056028965204802.jpg](https://piccdn3.umiwi.com/img/201805/28/201805280056028965204802.jpg)

图中按照黑人/白人、低分/中高分、再犯罪和没有再犯罪分成了八个区块，浅色的区块代表再次犯罪的人，深色区块代表没有再犯罪的人。为了方便说话，我给这些区块编了号码。

从图中我们可以看出四个事实。

 *第一，* 不管是黑人白人，每个分数分类内部的再犯罪率都是几乎一样的，B1/B2 = W1/W2，B3/B4 = W3/W4。分数很好地预测了再犯罪率，从这个意义上说算法是公平的。

 *第二，* 现在我们看看那些没有再犯罪的人，也就是 B2，B4，W2，W4，其中有人被打了高分有人被打了低分，打低分的（B2，W2）是准确的预测，打高分的（B4，W4）是冤枉的预测。

好，从图中可以直观地看出，其中黑人被冤枉的比例，也就是 B4/(B2+B4) = 42%；而白人被冤枉的比例，也就是 W4/(W2+W4) = 22%。这就是 ProPublica 说的种族歧视，“好黑人”被打高分的可能性比“好白人”大。

可这是为什么呢？关键在于下面两个事实。

 *第三，* 黑人的总体再犯罪率，高于白人。(B1+B3) 占所有黑人的比例是52%，而对白人，总的再犯罪率是39%。算法并不知道谁是黑人谁是白人，可是算法注意到了有一些人的再犯罪率更高。

 *第四，* 由于黑人的总体再犯罪率高，所以黑人的被打高分的比率一定也高。(B3+B4) 占所有黑人的比例是58%，而对白人来说这个比例是33%。

由此可见， **因为黑人总体再犯罪率高，黑人打高分的比例就必然高。每个分数代表的人群中被冤枉的比例是固定的，那既然黑人总体打高分的比例就高，那么其中被冤枉的黑人就一定更多。**

这一段如果听语音很难理解，我建议你对照图形仔细琢磨琢磨。要点在于这纯粹是一个数学现象，算法事先并没有针对黑人做任何特别的设定。

我再换个说法。假设现在有个外星人，他根本不知道地球上的人还分为黑人和白人，他以为所有人都是一样的。那么他拿到数据之后，会认为这是一套公平的打分系统。可是当我们把黑人白人的标签放进去之后，却发现黑人被冤枉的比例更高。

而个别黑人之所以吃亏，是因为黑人整体的再犯罪率高。

那怎么避免这个现象呢？从数学角度根本避免不了。如果你想保护黑人，给黑人少打一些高分，那你的分数系统就是不准确的，同样一个分数就不能代表同样的再犯罪率，法官就没法从这个分数系统中获得正确的参考意见。

 *所以到底什么叫公平？程序员追求的是分数系统的准确性，记者要求的是不能冤枉黑人，而从数学上来说，这两个要求不可能同时满足！*

## 4.大道理

《人工不智能》这本书的作者布鲁萨德认为程序员一味追求算法的准确度是一个认知“偏误”，它无形之中歧视了黑人。我觉得你很难说这应该叫“偏误”，这其实是一个根本的矛盾。

 *只要用过去的经验去预测未来，就一定有这个矛盾。产生经验的是一批人，要被预测影响的却是另外一批人。这就相当于新人要为前人犯的过错承担后果！*

其实这个矛盾在生活中普遍存在。比如在上海这样不存在重男轻女的大城市里，女生的考试成绩普遍比男生好。那如果根据这个规律，为了提高学校的总体成绩，在高一入学的时候尽量多录取女生，这不就是对男生的歧视吗？

你的经验很可能是准确的，但是你是在用以前人的表现去惩罚后来的人。你是在让一个人为不是他自己的行为付出代价。

这是一切基于经验的决策的本质缺陷。人工智能再厉害，只要是基于经验的，只要预测不是百分之一百准确，就一定会有人被冤枉。

我们今天发现的是一个非常微妙的被冤枉的情况。 *所谓“公平”，其实是你的主观选择。* 选择算法准确度的公平，你就会冤枉一些特定的黑人；选择不冤枉黑人，你的算法就不准确，你就会冤枉别的人。

算法可以根本不考虑种族，但种族就隐藏在数据之中。丑小鸭定理说一切分类都是主观的。有分类就会有歧视，此事古难全。 *人工智能能给我们的决策提供很大的方便，但社会还是这个社会，数学还是同样的数学，人工智能改变不了问题的本质。*

## 划重点

一切基于经验的决策都有本质缺陷。你的经验很可能是准确的，但是你是在用以前人的表现去惩罚后来的人。你是在让一个人为不是他自己的行为付出代价。人工智能再厉害，只要是基于经验的，只要预测不是百分之一百准确，就一定会有人被冤枉。

![https://piccdn3.umiwi.com/img/201805/27/201805271510155185287914.jpg](https://piccdn3.umiwi.com/img/201805/27/201805271510155185287914.jpg)

---

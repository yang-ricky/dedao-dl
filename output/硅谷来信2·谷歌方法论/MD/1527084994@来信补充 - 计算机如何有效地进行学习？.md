# 来信补充 | 计算机如何有效地进行学习？

今天的来信补充，是在《硅谷来信》的第245封信的基础上，稍做了些增补、迭代。

和你聊聊Alpha Go在软件上是如何学习下棋的。或者更广泛地讲，计算机是如何有效地进行学习的。我们先从目前比较热门的一个概念——机器学习（Machine Learning）说起。

机器学习，实际上是寻找一种数学模型，让这种模型符合它所要描述的对象。比如说我们要寻找一种描述天体运动的模型，让它符合太阳系行星的运动情况，今天这个模型就是开普勒－牛顿的椭圆形模型：太阳系中所有的行星和彗星围绕着太阳系的重心（和太阳的位置很接近但是有所差别），做椭圆运动。当然，这个模型内不同物体的运动速度和周期不同，这些数据被称为模型的参数。

有了这样一个模型，就可以知道今后太阳系中行星和彗星的运行规律了，哈雷就用它成功地预测了哈雷彗星未来回到人们视野的时间。在这个模型中，要得到模型的参数（比如每一个星体运动的长轴半径，运动一圈的周期），就要根据观察到的历史数据计算。

开普勒一辈子做的事情大抵如此，他前半辈子跟着老师第谷获得数据，后半辈子做人肉的机器学习——找到模型的参数，让他所预测的行星运动符合所观察到的真实情况。

我们在机器学习中有时用“契合”来形容模型描述的情况和真实情况的接近程度，契合度越高，误差越小，当然也就越好。在数学上有一个“切比雪夫大数定律”，保证了只要数据量足够大，这种契合度就能接近100%。当然，因为开普勒是做“人肉”学习，速度很慢，花了整整半辈子，而今天的计算机做这件事情连万分之一秒都不需要。

机器学习的过程其实和开普勒的工作类似，有了一堆的数据，计算出一个合适的模型。

机器学习根据使用的数据不同，区分成了不同的类型。如果使用到的数据是杂乱无章的，机器想从这些数据中学习并得到规律，就被称为“无监督的学习”，也就是说没有人的监督。

举个例子，假如我们只是把晚上天空的星图拍下来，不标注上每一颗星星的名称，要让计算机自动找出每一颗星星运动的轨迹，这就是 **无监督的学习。**

反过来，如果由人来标识出每一颗星星的名称，再让计算机学习，那就是 **有监督的学习，** 也就是说机器学习是在人的监督下进行的。

这两种机器学习的方式各有千秋， *无监督的学习很容易获得大量的数据，* 但是由于没有做人工标识，因此计算机相当于自己在找方向，这样机器学习出来的模型就可能不准确，比如把土星的运动轨迹和木星的相混淆。

有监督的学习则相反， *一方面输入的数据比较精确，* 相当于利用了人类的智力，有了一个大致的方向。但是因为人工标识数据不仅成本高，而且很难得到足够量的数据。这种机器学习方法提高到一定程度，就再也提高不了了。

好了，有了这些背景知识的铺垫，现在可以讲回到Alpha Go的学习方法了。它一开始使用的是有监督的学习，也就是学习那几十万盘人类高手对弈的棋谱。因为那些下棋的人都是高手，因此Alpha Go假定他们下的都是好棋。

无论是李世石还是柯洁，一辈子研究的顶级对弈恐怕不过几千盘，也就是Alpha Go的1%，下不过Alpha Go情有可原。但是，人类积累了几千年，一共可查得到的高手对弈也就那么一点点，Alpha Go要是沿着这种思路去学习，改进、进步的速度就太慢了。

另外，和九段棋手学棋，哪怕学得再多，恐怕到10段也就到头了，这样Alpha Go的棋艺就不能比人类高出一大截。

当然，Alpha Go也不能用没有监督的学习，因为围棋的变化太多，虽然计算机速度快，但在围棋领域，试错是试不过来的。

因此，Alpha Go就采用了一个新的学习方法，就是所谓的 **“强化学习”** ( Reinforcement Learning)。它的方法其实也很简单，就是计算机在没有人为给定方向的条件下，自己试着走一个方向，然后由人设定的一些原则告诉它好不好，也就是说需要有一个判断价值的反馈信号送还给机器学习系统。

接下来，我就用无人驾驶汽车的学习方法为例来说明这三种机器学习之间的区别。

2006年Google开始做无人驾驶之前，先开展了街景地图项目，不仅获取了全部的道路和街景数据，而且获得了所有时段主要道路的交通状况数据，然后Google对这些数据作了处理，并且和GPS的数据对照做了标识，个别数据还是手工处理的。

因此，无人驾驶汽车的很多模型是利用这些标识好的数据事先进行了有监督的学习。当然，Google无人驾驶汽车上路后，还在不断地进行数据采集，以便让模型越来越准确。由于Google 无人驾驶汽车上路之前的起点和终点是设置好的，路线上的边界也是设定好的，目标是明确的，因此它进行的还是有监督的学习。

等到特斯拉搞无人驾驶汽车时，情况就不同了，它事先没有街景、地图等标识好的数据，那么它是怎么做的呢？它就根据人的反馈不断进行学习。比如说它的辅助驾驶功能，一开始基本上只能靠着车道识别的功能，沿着划定得非常清晰的车道行驶。但是到了车道不是很清楚的地方，或者分叉路时，特斯拉就傻眼了，它有一半的可能性要开错车道。

不过这时人会给它一个反馈信息：如果开得对，人就不干预，如果开错了，人会马上手动控制。这样虽然没有标识告诉它怎么开是对的，但是这种反馈接受多了，它将会朝着更好的方向进化。这就是强化学习。

今天，很多无人驾驶系统其实已经安装在某些由人驾驶的汽车上，它不影响人的开车行为。但是在人开车的同时，这个无人驾驶的模块也在模拟开车，这样它就获得了增强学习的强化信号，如果自己的操作符合安全到达的最终目的，就会得到奖赏。这如同我在《硅谷来信》第18封信中介绍期望值最大化时讲到的，这样一来，人工智能就会往完全到达目的地这个期望值优化自己。

因此，可以认为强化学习虽然没有事先人为标定的数据作参考，但是对于它学习的行为会有反馈意见作指导，如果走错了路，偏离了设定的期望值，可以纠正过来。

现在我们再次回到Alpha Go的问题上来，如果它只使用人为标注的数据，即人下棋的棋谱，不仅数据有限，而且就算训练出来的模型和人的数据的契合度达到100%，最多也就是比人的水平高那么一点，发挥更稳定一点，仅此而已，因为人没有下过的棋它学不到。

当然，让它没有目的地试错也不现实。因此Google采用了强化学习的办法，在它作尝试的时候，给它一点反馈，告诉它这个方向对还是不对，这样它不至于胡思乱想。当然，这个反馈怎么得来，则是一门艺术，因为对于人类没有下过的棋，如何判断好坏呢？

对此，Google的Alpha Go团队做了一个所谓的“代理”（Agent），它是一个程序，能够判断棋盘上大致的胜率，每走出若干步之后，如果这个代理认为对于胜率有所提升，那么就说明这个方向是对的，在机器学习上就予以鼓励，否则就予以处罚。在对阵李世石和对阵柯洁之间的一年中，Alpha Go的团队一直在通过强化学习改进下棋的算法，核心是改进这个代理所给出反馈的准确性。

至于Alpha Go是如何判断胜率的，Google内部的同事讲了一些细节。他说，Alpha Go的训练程序其实很难判断单一一步的胜率变化，但是能够判断若干步累计下来的整体变化，然后它再反过来给每一步走法一个反馈。通过这种方式，Alpha Go就在尝试很多人之前没有尝试过的做法，才实现了超越人类。关于Alpha Go强化学习之后的新版本Alpha Zero，请读去年底的来信《解析人工智能阿尔法元》。

Alpha Go的强化学习，其实从原理上讲和我们日常很多做事的方法是一致的。比如教育小孩时，他如果最近表现比较好，就给他一些鼓励，久而久之小孩就被训练得往好的方向发展。

最后为了方便理解，我把有监督的学习，无监督的学习和增强学习通过下面这个比方再总结一下。

 *有监督的学习就如同凡事你都学习你的上级之前的做事方法，* 但是水平也不会高出他们太多。

 *无监督的学习就是没人管，* 自己做，到年底上级根据你的表现给你奖金，奖金就是你做事的目标函数。

 *强化学习就是没人管你怎么做，如果做得好，上级及时给你反馈；如果做坏了，对你进行批评。*

我有时在想，人之间的道理和物之间的道理很多时候是相通的。中国人喜欢讲天道，可能这些规律就是一种天道吧！

![https://piccdn3.umiwi.com/img/201805/17/201805172015474116317127.png](https://piccdn3.umiwi.com/img/201805/17/201805172015474116317127.png)

![https://piccdn3.umiwi.com/img/201805/17/201805172016091848202167.jpg](https://piccdn3.umiwi.com/img/201805/17/201805172016091848202167.jpg)

---

# AI专题11:GPT的底牌和命门

这一讲开始我们专注讨论GPT。前面讲了大型语言模型有开悟，有涌现，有思维链，所以才有现在如此神奇的各种功能。但我们还需要进一步理解GPT：它跟人脑到底如何对比？它有什么限制？有没有它不擅长的东西？

身处历史变局时刻，GPT的进展非常快。各种产品、服务，学术论文层出不穷，进步是以天来计算，一个月以前的认识都可能已经过时了。不过我们这讲用的书很厉害，史蒂芬·沃尔夫勒姆（Stephen Wolfram）的《ChatGPT在做什么…以及它为什么好使》（What Is ChatGPT Doing ... and Why Does It Work?），2023年3月9日刚刚出版。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436799670543244/040317.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436799670543244/040317.png)

这本书不会过时。因为它讲的不是GPT的一般功能，而是数学原理和哲学思辨——数学和哲学是不会过时的。

沃尔夫勒姆你可能比较熟悉，我们专栏之前专门讲过他 [1]。他是一个神人。他发明了Mathematica软件，他做了WolframAlpha网站，他搞了一个计算语言叫沃尔夫勒姆语言，他对整个物理学提出了全新的看法。你要让我列举当今世界上活着的最聪明的三个人，那其中必定有沃尔夫勒姆——而且我还不敢肯定另外两个是谁。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436843693896028/040317.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436843693896028/040317.png)

GPT和目前市面上所有的AI，本质上都是神经网络。沃尔夫勒姆关注神经网络已经四十多年了，他早在1983年就自己编程研究过神经网络，他最近用GPT做了很多研究。他这本书得到了OpenAI CEO山姆·奥特曼（Sam Altman）的背书，说是他所见过最好的解释。

事实上，沃尔夫勒姆不但讲清楚了GPT的底牌和命门，而且提出了一个可谓惊世骇俗的洞见。

✵

我先给你演示个小案例，加深你对GPT的认识。

我让GPT-4做了个最简单的计算题，纯粹是我随手打的：1231 × 434523 + 323 × 34636 等于多少？

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436880201118044/040317.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436880201118044/040317.png)

GPT-4煞有其事地算了一番，给出的结果是546106021。但是你随便找个计算器算算，正确答案应该是546085241。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436915634642536/040317.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436915634642536/040317.png)

这是怎么回事呢？GPT-4有强大的推理能力，我让它做奥数题它有时候都能做对，怎么这么简单的计算题它做不对呢？

当然它也不是什么计算都不会。你要让它算个25+48，它肯定能做对……问题是对于数字特别长的计算，它就不行了。

根本原因在于，GPT是个语言模型。它是用人的语言训练出来的，它的思维很像人的大脑——而人的大脑是不太擅长算这种数学题的。让你算你不也得用计算器吗？

 **GPT更像人脑，而不是像一般的计算机程序。**

✵

在最本质上，语言模型的功能无非是对文本进行「合理的延续」，说白了就是预测下一个词该说什么。

沃尔夫勒姆举了个例子，比如这句话：“The best thing about AI is its ability to……（AI最棒的地方在于它具有……的能力）”下一个词是什么？

模型根据它所学到的文本中的概率分布，找到五个候选词：learn（学习），predict（预测），make（制作），understand（理解），do（做事），然后它会从中选一个词。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436969321751436/040317.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804436969321751436/040317.png)

具体选哪个，根据设定的「温度」有一定的随机性。就这么简单。GPT生成内容就是在反复问自己：根据目前为止的这些话，下一个词应该是什么？

输出质量的好坏取决于什么叫「应该」。你不能只考虑词频和语法，你必须考虑语义，尤其是要考虑在当前语境之下词与词的关系是什么。Transformer架构帮了很大的忙，你要用到思维链，等等等。

是，GPT只是在寻找下一个词；但正如奥特曼说过，难道人不也*只是*在生存和繁衍吗？最基本的原理简单，可是各种神奇和美丽的事物却可以从中产生。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437035893726824/040317.jpeg](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437035893726824/040317.jpeg)

训练GPT的最主要方法是无监督学习：先给它看一段文本的前半部分，让它预测后半部分是啥。这样训练为啥就管用呢？语言模型为什么跟人的思维很接近？为了让它有足够的智慧，到底需要多少个参数？应该喂多少语料？

你可能觉得OpenAI已经把这些问题都搞明白了，故意对外保密——其实恰恰相反。沃尔夫勒姆非常肯定地说，现在没有科学答案。没人知道GPT为什么这么好使，也没有什么第一性原理能告诉你模型到底需要多少参数，这一切都只是一门艺术，你只能跟着感觉走。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437062737290124/040317.jpeg](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437062737290124/040317.jpeg)

奥特曼也说了，问就是上天的眷顾。OpenAI最应该感恩的，是运气。

✵

沃尔夫勒姆讲了GPT的一些特点，我看其中有三个最幸运的发现——

 **第一，GPT没有让人类教给它什么「自然语言处理（NLP）」之类的规则。** 所有语言特征，语法也好语义也好，全是它自己发现的，说白了就是暴力破解。事实证明让神经网络自己发现一切可说和不可说的语言规则，人不插手，是最好的办法。

 **第二，GPT表现出强烈的「自组织」能力，也就是我们前面讲过的「涌现」和「思维链」。** 你不需要人为给它安排什么组织，它自己就能长出各种组织来。

 **第三，也许是最神奇的一件事情是，GPT用同一个神经网络架构，似乎就能解决表面上相当不同的任务！** 按理说，画画应该有个画画神经网络，写文章应该有个写文章神经网络，编程应该有个编程神经网络，你得去分别训练——可是事实上，这些事情用同一个神经网络就能做。

这是为什么？说不清。沃尔夫勒姆猜测，那些看似不同的任务，其实都是「类似人类」的任务，它们本质上是一样的——GPT神经网络只是捕获了普遍的 "类似人类的过程"。

这只是猜测。鉴于这些神奇功能目前都没有合理解释，它们应该算作是重大科学发现。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437117498061148/040317.jpeg](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437117498061148/040317.jpeg)

 *这些是GPT的底牌：它只是一个语言模型，但同时，它很神奇。*

✵

那GPT为什么算数学就不太行呢？沃尔夫勒姆讲了很多，下面我用一张图给你简单概括一下。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437148636618344/040317.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437148636618344/040317.png)

我们用三个集合代表世间的各种计算，对应于图中三个圆圈。

大圈代表一切计算。我们可以把自然界中所有现象都理解成计算，因为底层都是物理定律。其中绝大多数计算过于复杂——比如我们专栏讲过「一个质子就是一片海」——以至于我们连方程都写不全，不管是用大脑还是用计算机都不能处理，但我们知道那也是计算。

大圈内部，左边这个小圈，代表神经计算，适合神经网络处理。我们的大脑和包括GPT在内当前所有的AI，都在这里。神经计算善于发现事物的规律，但是对数学问题的处理能力有限。

大圈内部右边这个小圈代表「形式逻辑」，数学就在这里。这里的特点是精确推理，不怕繁杂，永远准确。只要你有方程有算法，这里就能兢兢业业地给你算出来。这是特别适合传统计算机的领域。

不论是人脑、GPT还是计算机都处理不了世间所有的计算，所以两个小圈远远不能覆盖整个大圈。我们搞科学探索，就是要尽可能地扩大两个小圈的范围，进入大圈中未知的领地。

人脑和GPT也可以处理一部分形式逻辑，所以两个小圈有交集；但是我们处理不了特别繁杂的计算，所以它们的交集并不大。

那你说有没有可能将来GPT越来越厉害，让左边的小圈完全覆盖右边的小圈呢？那是不可能的。沃尔夫勒姆认为，语言思考的本质，是在寻求规律。而规律，是对客观世界的一种压缩。有些东西确实有规律你可以压缩，但有些东西本质上就没有规律，不能压缩。

比如我们专栏以前讲过沃尔夫勒姆发明的一个游戏，其中有个「第30号规则」，就是「不可约化的复杂」[1]：你要想知道将来是什么样子就只能老老实实一步步算出来，不能“概括”。

这就是为什么GPT算不好繁杂的数学题。GPT跟人脑一样，总想找规律走捷径，可是有些数学题除了老老实实算没有别的办法。

更致命的是目前为止，GPT的神经网络是纯粹的「前馈（feed-forward）」网络，只会往前走、不会回头、没有循环，这就使得它连一般的数学算法都执行不好。

 *这就是GPT的命门：它是用来思考的，不是用来执行冷酷无情的计算的。*

✵

这样说来，虽然GPT比人脑知道的更多、反应更快，但作为神经网络，它并没有在本质上超越人脑。

说到这里，沃尔夫勒姆有个洞见。

用这么简单的规则组成的神经网络就能很好地模拟人脑——至少模拟了人脑的语言系统——这说明什么呢？老百姓可能说这说明GPT很厉害——而沃尔夫勒姆却说，这说明人脑的语言系统并不厉害。

 *GPT证明了，语言系统是个简单系统！*

GPT能写文章，说明在计算上，写文章是一个比我们想象的*更浅*的问题。人类语言的本质和背后的思维，已经被一个神经网络给捕捉了。

在沃尔夫勒姆眼中，语言无非就是由各种规则组成的一个系统，其中有语法规则和语义规则。语法规则比较简单，语义规则包罗万象，包括像“物体可以移动”这样默认规矩。从亚里士多德开始就一直有人想把语言中所有的逻辑都列出来，但是从来没人做到——可是现在GPT给了沃尔夫勒姆信心。

沃尔夫勒姆说你GPT能做的，我也能做。他打算用一种计算语言——也就是「沃尔夫勒姆语言」——取代人类语言。这么做有两个好处。一个是精确性，人的语言毕竟有很多模糊的地方，不适合精确计算；另一个则更厉害：沃尔夫勒姆语言代表了对事物的“终极压缩”，代表了世间万物的本质……

因为我听说过哥德尔不完备性定理，所以我不太看好他这个雄心壮志——但是我能想到的人家肯定早就想过了，所以我没有反对意见，我只是想告诉你这些。

✵

总而言之，GPT的底牌是它虽然结构原理简单，但是已经在相当程度上拥有人脑的思维。现在还没有一个科学理论能完整解释它为什么能做到，但是它做到了。GPT的命门也是因为它太像人脑了：它不太擅长做数学计算，它不是传统的计算机。

这也解释了为什么GPT很擅长编程，却不能自己执行程序：编程是语言行为，执行程序是冷酷计算行为。

理解了这些，我们研究怎么调教GPT就有了一点理论基础。但我现在想说的是，GPT的所谓命门其实很容易弥补。

你给它个计算器不就行了吗？你另外找台计算机帮它执行程序不就行了吗？最近OpenAI允许用户和第三方公司在ChatGPT上安装插件，恰恰就解决了这个问题……所以GPT还是厉害。

但是沃尔夫勒姆让我们认识到了GPT的根本局限性： *神经网络的计算范围是有限的。* 我们现在知道，将来就算AGI出来，也不可能跳出神经计算和形式逻辑去抓取大自然的真理——科学研究终究需要你跟大自然直接接触，需要调用外部工具和外部信息。

希望这一讲能对GPT有所祛魅。它的确是不可思议地强，但它远远不是万能的。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437190512567180/040317.jpeg](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040317/1804437190512567180/040317.jpeg)

注释

[1] 精英日课第一季，一个神人的世界观

## 划重点

1.GPT的底牌：它只是一个语言模型，但同时，它很神奇。
2.GPT的命门：它是用来思考的，不是用来执行冷酷无情的计算的。
3.我们要认识GPT的根本局限性：神经网络的计算范围是有限的。

---

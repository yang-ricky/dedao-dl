# 伊丽莎效应

随着AGI即将到来，我们最近很关心AI什么时候、如何才能有情感能力，到时候我们会不会把AI当成真人。这些讨论中可能忽略了一点，那就是实际上人们很容易把机器当真人。

早在1966年——那时候还没有个人电脑——麻省理工学院有个计算机科学家叫约瑟夫·维森鲍姆（Joseph Weizenbaum），就写出来一个很简单的聊天机器人，叫伊丽莎（ELIZA）[1]。你可以用打字的方式跟它互动。伊丽莎的角色设定是心理医生。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040618/1804719587833504652/040618.jpeg](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040618/1804719587833504652/040618.jpeg)

其实维森鲍姆只是用了一些最简单的语言处理，伊丽莎根本谈不上是AI。但是他做得很巧妙。伊丽莎会寻找用户输入的话中间的关键词，做出特定反应。比如你说我最近有点“抑郁”，它马上就会说“I'm sorry to hear that……”好像真听懂了一样。而如果你的话里没有它需要的关键词，它也会表现得很冷静，说“请你继续”、“请告诉我更多”，你看是不是很像真的心理医生。

这让我想起大概是1997年，我上大学的时候，也曾经跟一个设定是心理医生的聊天机器人对话，也许就是伊丽莎的某个后续版本。我为了让对话更有意思一点就假装自己有心理问题，我说我最近心情非常不好，也不知道为啥。然后那个机器人说了一句惊世骇俗的话，让我的情感波动立即飙升——它说，是不是跟你的性生活有关？

程序其实啥也不懂，但是它特别能调动你的情绪……我有点怀疑真的心理医生是不是也是这样工作的。不过那都不重要，重要的是，人们被伊丽莎迷住了。

✵

维森鲍姆写这个程序一半是做研究，一半算是做着玩的，他让同事们试用伊丽莎。结果他万万没想到，试用者纷纷认真了。同事们会跟伊丽莎进行长时间对话，向她透露自己生活中一些私密的细节，就好像真在接受心理治疗一样。

那可都是麻省理工学院的教授啊。

尤其有个女秘书，特意要求在房间里没有其他人的情况下跟伊丽莎聊。据说她聊着聊着还哭了。

有的同事建议让伊丽莎处理一些真实世界的问题，他们相信伊丽莎有深入理解和解决问题的能力。

维森鲍姆反复告诉这些人，那只是一个计算机程序，并没什么真能力！但是人们不为所动，他们是真的觉得伊丽莎能理解他们。他们很愿意相信伊丽莎有思维能力，他们不由自主地赋予伊丽莎人的特性。

1976年，维森鲍姆出了一本书叫《计算机势力与人的理性：从判断到计算》（Computer Power and Human Reason: From Judgment to Calculation），讲了这件事。从此之后，人们就把这个现象叫做「 *伊丽莎效应（The Eliza Effect）* 」，意思是人们会无意识地把计算机给拟人化。

✵

伊丽莎效应的根本原因在于我们对计算机说的一些话做了过度解读 [2]。它根本没有那么深的意思——甚至根本就没有任何意思，但是你脑补出了深意。

现在有了AI，伊丽莎效应就更容易发生了。

Google有个大语言模型叫LaMDA，可以进行开放式对话，有一定的语言理解能力。结果它自家的一个工程师跟它聊了一段时间之后，认为它已经有了人的意识。那个工程师从Google离职之前还给公司两百人的机器学习群发了个邮件，说LaMDA有意识，它只想好好帮助世界，请在我不在的日子里好好照顾它……

Bing Chat刚出来的时候，对所用的GPT没有很多的限制，以至于有时候会说些不该说的话。有的用户就故意刺激它，引导它说出自己有个秘密的名字叫Sydney……最后越聊越深入，聊出了很激烈的情绪，感觉悉尼好像发怒了。

更别说之前有个电影叫《她》，讲一个人爱上了他的AI助理。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040618/1804719631856857436/040618.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040618/1804719631856857436/040618.png)

最近还有个传闻说一个比利时人跟一个也叫伊丽莎的聊天机器人聊了六个星期之后自杀了 [3]。

所以现在更迫在眉睫的问题似乎不是AI到底有没有意识，而是人们过于愿意相信AI有意识 [4]。

但这种心态并不是AI、也不是计算机带给我们的。这叫「 *拟人化（anthropomorphism）* 」。我们天生就爱把非人的东西拟人化。根本不需要是AI，生活中的小猫小狗，甚至是一个玩具、一辆汽车，都有人当它是人，有意无意地觉得它有情感，有性格，有动机，有意图。

日本已经在用机器人陪护老人。那些机器人长得一点都不像人，但是很多老人会把它们当成自己的孩子……

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040618/1804719668364123752/040618.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040618/1804719668364123752/040618.png)

研究者认为 [5]，只要你有最基本的关于人的认识，你在某个物体身上寻找像人一样的行为和动机，再加上你需要社交互动，你就很容易把这个东西给拟人化。

有些人担心伊丽莎效应会不会带来什么社会问题，在我看来这种担心目前为止是不必要的。人人都会在某些时候把某些物体拟人化，这完全是健康的，AI并不特殊。用ChatGPT找陪伴感，乃至于产生了一点情感依赖，其实都不是什么大问题——没有理由认为那些追星的粉丝对明星的情感依赖就更健康。宅男再喜欢GPT，也不至于因为爱上了AI而跟妻子离婚……而且所谓“爱上AI”的人其实是很少的。我不相信伊丽莎效应能威胁人的存在价值。

更有意思的问题怎样才能判断，现在到底是AI真有意识了，还是你自己产生了伊丽莎效应……

✵

不过这一讲最想说的不是那些。伊丽莎效应也许能让我们反思，我们的拟人化倾向会不会稍微有点泛滥，乃至于给自己带来麻烦。

麻烦在于，拟人化，有时候不是让你觉得一个东西可爱，而是让你对这个东西产生了恨意。

比如你正在电脑上工作，这个工作很重要而且截止时间马上要到了，可你的电脑突然崩溃了，也许是因为Windows升级。那你会不会有一种强烈的感觉，觉得电脑是不是在跟你作对，微软公司是不是太邪恶了？

当然，微软的确是挺邪恶的……但这个道理是，我们有时候会给明明没有动机和意图的事物安上动机和意图。

再比如说，孩子在家里跑，不小心撞倒了桌子上，哭了。父母往往会一边安慰孩子一边打那个桌子，意思是给孩子出气。可是孩子再小也应该明白，桌子没有意图！人家老老实实连动都没动。

这个规律是因为我们总想为自己的麻烦找一个怪罪的对象，所以我们会很方便地把一个什么东西给拟人化。

✵

再举个例子。你开车遇到了拥堵，本来就很不耐烦，这时候如果正好赶上有人用不太规范的方式超你车，你就可能会产生路怒，认为那辆车是在针对你。当然，那辆车确实有个人在开，但是，你们双方都在车里连对方的脸都看不清楚，他根本不知道谁是谁！这其实也是一种拟人化，这是给一个没有意图的局面赋予了意图，可以说是「 *对局面的拟人* 」。

对局面的拟人实在太普遍了。

你到一家餐馆吃饭，排了很长的队才排到，本来就筋疲力尽，一看服务员的态度还不太好，你真想打一架。可问题是你要知道，服务员态度不好是因为他已经站了一整天了，他很累，他并不是在针对你。

你去一家机关单位办事，发现是门难进、脸难看、事难办，你火冒三丈，跟一个工作人员差点打起来。但你要知道那就是普通的官僚主义，本县哪个机关、对谁都一样，他们只是在做他们一直做的事情。

我们专栏以前讲过「汉隆剃刀」[6]，说能用愚蠢解释的，就不要用恶意——现在我们可以把这个道理推广一下： *能用局面和系统解释的，就不要拟人化。*

✵

人们总是不自觉地把系统的事情归因于个人。人们都说乔布斯特别有创造力，说乔布斯发明了iPhone——但iPhone是乔布斯发明的吗？iPhone难道不是苹果公司无数个工程师一起研发出来的吗？

乔布斯去世后，蒂姆·库克成为苹果CEO，结果在很长一段时间内，苹果每次发布新产品都有人说没了乔布斯苹果就不行了。可事实明明是每一代iPhone都比之前的更好。

现在马斯克又被普遍认为是最聪明的人，说他发明了这个发明了那个……而事实是马斯克只是一个领导而已：他要做的不是自己发明什么东西，而是找最聪明的人替他发明东西。

 *把公司行为解释成领导的意图，这也是一种拟人化。*

Facebook出了一些涉及用户隐私的问题，人们就把扎克伯格描绘成了一个坏人。可是有没有一种可能，扎克伯格比任何人都不希望他的公司作恶，他只是控制不了局面？如果你运营一个有几亿人的网络社区，你很难控制局面。

把宠物当人，把玩具当人，把AI当人是拟人化；把局面当成人，把系统当成人，把公司当成人，也可以说是一种伊丽莎效应。

✵

拟人化给我们平添了不少烦恼，那么借着AI这个热度，我们也许可以稍微做点反思。我们能不能在生活中搞搞「反拟人化」呢？

核心思想是「不是针对你」，英文叫「nothing personal」或者「don't take it personal」。这个事儿只是事儿赶事儿赶到了这里，不是针对你，不是出于个人恩怨，没有别的意思！

比如你在工作中需要指出同事或下属的一个错误，你可以先说一句不是针对你。我指出这个错误只是为了把事情做好。其实这种话让AI去说可能更好，因为被人指出错误的时候真是很难相信对方不是在针对自己，拟人化倾向实在太强烈了……但不论如何，事先说一句总比不说强。

反驳上司的一个观点，拒绝别人的一次邀请，参加一场跟朋友之间的竞争，这类场合都特别需要去拟人化。尤其是当你被反驳、被拒绝、被竞争的时候。

我认为多跟ChatGPT聊天可以提高我们去拟人化的能力。GPT，我们目前姑且还可以认为，它没有自我。它只是在预测下一个词该说什么而已，它并不真的认识你，更谈不上针对你。

那我们推而广之，如果能把路上司机、餐馆服务员、政府工作人员、上司、同事和朋友都偶尔当成一次AI，你会少很多烦恼。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040618/1804719702723879820/040618.jpeg](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023040618/1804719702723879820/040618.jpeg)

注释

[1] https://99percentinvisible.org/episode/the-eliza-effect/

[2] http://scihi.org/joseph-weizenbaum-eliza/

[3] https://www.gamingdeputy.com/an-ai-is-suspected-of-having-pushed-a-man-to-suicide/

[4] https://theconversation.com/ai-isnt-close-to-becoming-sentient-the-real-danger-lies-in-how-easily-were-prone-to-anthropomorphize-it-200525

[5] Epley, N., Waytz, A., & Cacioppo, J. T. (2007). On seeing human: A three-factor theory of anthropomorphism. Psychological Review, 114(4), 864–886.

[6] 精英日课第四季，能用愚蠢解释的，就不要用恶意

## 划重点

1.「伊丽莎效应」，意思是人们会无意识地把计算机给拟人化。
2.把宠物当人，把玩具当人，把AI当人是拟人化；把局面当成人，把系统当成人，把公司当成人，也可以说是一种伊丽莎效应。
3.能用局面和系统解释的，就不要拟人化。

---

# 局面：目前为止的GPT世界观什么样？

你好，这里是《万维钢·AI前沿》课的加餐内容。距离GPT浪潮引爆已经过去了几个月，我的一些认识正在趋于稳定，这一讲咱们梳理一下目前的局面。

面对一个新事物，你一上来就觉得它很厉害，然后就到处宣扬它有多厉害，这是一个有点冒险的行为：因为可能浪潮很快就过去了，事实证明这东西并没有当初你想象得那么厉害，你就会觉得当时自己挺傻的。

但我认为这才是对的。一见到新事物就很激动，一惊一乍，恰恰证明你心仍然会澎湃，你没有陷入认知固化。这比看见什么东西都用自己老一套的世界观去解释，说啊，这我三十年前就搞明白了……要强得多。要允许自己继续长进，你就得敢于让人说你傻才行。

GPT绝不是三十年前的观念能理解的东西。最近几年的几个关键突破已经彻底改变了神经网络和语言模型研究的面貌。

以我之见，2021年以前出版的所有讲AI的书，现在统统都过时了。

✵

AI和人脑还有本质区别吗？语言模型到底是个什么东西？

我在《精英日课》第四季的文章中，发过一张照片，照片里是奥巴马和他的几个同僚在一个走廊里，有个老兄站在体重秤上称体重——他不知道的是，奥巴马在他身后，把脚踩在体重秤上，给他加了一点重量。周围的人都在笑。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689134047362112/051913.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689134047362112/051913.png)

计算机视觉专家安德烈·卡帕蒂（Andrej Karpathy）在2012年10月的一篇博客文章用过这张照片，当时他的论点是要想让AI能看懂这张照片，非常非常困难。

AI要想理解这个情景的有趣之处，必须先得具备一些「常识」：体重秤是干什么的、多一只脚踩在体重秤上会让读数增加，奥巴马是在搞恶作剧，因为现代人都希望减肥，害怕体重增加，以及奥巴马以总统之尊开这个玩笑很有意思……这些常识不会被系统性地列举在哪本书里，都是我们人类日用而不知的东西，是「隐性知识（tacit knowledge）」。

你怎么才能教会AI这些隐性的常识？

有一位计算机科学家叫梅拉妮·米歇尔（Melanie Mitchell），是侯世达的学生，我最早了解进化算法就是从她的书里学的。米歇尔2019年出了本书叫《AI 3.0》（Artificial Intelligence: A Guide for Thinking Humans），也谈到了这个问题。当时她用了大量的例子说明教AI常识有多么困难……

确实困难。你总不能把人类所有的常识都一条一条写下来，编进程序让AI学习，那根本就是写不完的。对吧？

所以我们都在感叹这个任务之难。

……殊不知，那一切的感叹，现在都已经过时了。

✵

GPT-4，能看懂那张照片。

GPT-4刚出来那天，OpenAI的手册里就有一个例子。在一张图片中，一个本来用于连接老式显示器的VGA插头被插到了手机上，这是一个错位笑话——

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689151227230804/051913.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689151227230804/051913.png)

而GPT-4看懂了，它明确说出了这张图的笑点在哪。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689167333400128/051913.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689167333400128/051913.png)

有人立即想到了卡帕蒂那张关于奥巴马和体重秤的照片，就在推特上问卡帕蒂，说能不能把那张照片让GPT-4试试？卡帕蒂马上跟帖说我们已经测试了，它看懂了，把它解出来了 ——

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689179144518720/051913.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689179144518720/051913.png)

卡帕蒂唯一担心的是可能OpenAI用那张照片训练过GPT……但我觉得这个担心是多余的，从各种表现来说，GPT-4完全拥有看懂这样的图的能力。

现在你完全不应该为此感到惊奇。我们前面讲了，我曾经问ChatGPT，棒球棒能不能放进人的耳朵里？为什么孙悟空的金箍棒可以放到耳朵里？它回答得很好，它有常识。

可它是怎么看懂的呢？这AI怎么就学会了人类的常识呢？

✵

2023年三月份，英伟达CEO黄仁勋和OpenAI首席科学家伊利亚·苏茨科弗（Ilya Sutskever）有一个对谈。在这个对谈中，苏茨科弗进一步解释了GPT是怎么回事儿。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689199545612884/051913.png](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689199545612884/051913.png)

苏茨科弗说，GPT的确只是一个神经网络语言模型，它被训练出来只是要预测下一个单词是什么——但是，如果你训练得足够好，它就能很好地掌握事物之间的各种统计相关性。而这就意味着神经网络真正学习的，其实是「世界的一个投影（a projection of the world）」——

神经网络所学习的是越来越多的世界、人们、人类境况的方方面面，包括他们的希望、梦想、动机，以及他们之间的互动和我们所处的各种情境。神经网络学会了对这些信息进行压缩、抽象和实用的表示。这就是通过准确预测下一个词汇所学到的内容。而且，预测下一个词汇的准确性越高，这个过程中的保真度和分辨率就越高。

换句话说，GPT学的其实不是语言，而是语言背后的那个真实世界！

我打个比方，禅宗有本书叫《指月录》，用的是当初六祖慧能的一段典故。慧能说真理就如同是月亮，而佛经那些文本就如同是指向月亮的手指：你可以顺着手指去找月亮，但你想要的不是手指而是月亮。现在训练GPT用的那些语料就是手指，而GPT抓住了月亮。

这就是为什么GPT有了常识。那是它自己从无数语料中摸索出来的。

那你说，难道单凭读文本就能抓住月亮吗？也许是的，或者至少在相当程度上可以。要不然呢？我们人类读书不也是这么干的吗？

✵

也许你需要更有悟性，也许你只需要读得多。

多了，就不一样。多会导致「涌现」。

据OpenAI总裁格雷格·布罗克曼（Greg Brockman）在四月份的一次TED演讲中说，最早的一次关键突破是发生在2017年。当时OpenAI的一个工程师用亚马逊商品评论训练了一个很简单的语言模型，那个模型明明分析的是*句法*，可是却自动获得了*语义*的感觉，能够判断和设定一段评论中的情感是正面的还是负面的。

然后GPT用上了Transformer架构。

然后在2021年前后，GPT有了我们前面讲过的「开悟」和「涌现」，自动拥有了「少样本学习」的能力，长出了推理能力，有了思维链。

还有一个很有意思的特点。跟ChatGPT对话，你用英文提问还是用中文提问，只要那个问题不是直接跟中国文化、中国历史相关的，它的回答质量就几乎是一样的。当然如果你问中国的事儿，用中文提问会更好，可能因为只有中文能准确表达；但对于一般的问题，比如科学问题，用哪种语言提问对GPT来说没啥区别。这是为什么呢？

一开始我以为是GPT先把中文翻译成英文，再用英文去思考，再翻译回中文回答。后来我越用越感觉不是这样。它并没有专门的翻译步骤。它不管你输入的提示语是哪种语言，都是直接从同一个神经网络中的同样的地方走，语言只是个表达界面而已。

这就如同我的大部分物理专业知识是用英文学的，但是你问我一个物理问题，你用中文还是英文对我没什么区别。我说不清我是用英文还是用中文思考物理，我没有念念叨叨地背诵什么论文，我是直接思考。

也就是说，GPT这个语言模型，抓住了语言背后的那个东西。

在三月份的一个播客访谈中，苏茨科弗还举了个例子。他说哪怕没有多模态功能，GPT单纯从文本上学习，也已经对颜色有了很好的理解。它知道紫色更接近蓝色而不是红色，知道橙色比紫色更接近红色……

它在不同的语料中读过那些颜色，但它记住的不是具体的语料，而是语料所代表的世界。它默默地掌握了那些颜色之间的关系。

✵

GPT的训练不是在简单地背课文，它是在通过手指去感受月亮。

这是几个月以前我们完全没有想到的事情，这是世界观级别的改变。

AI单凭文本，就掌握了世界的常识。是，语言只是真实世界的一个不完整的表示，很多事情在字里行间没说出来；但是，仅仅通过语言，神经网络也能抓住一点背后的东西。

会解释，能推理，能看懂图片中的笑话，会写文章会编程。如果这还不叫理解，什么才是理解？

事情正在向「AI和人脑没有本质区别，语言模型就是真实世界本身的投射」这个方向演进。

GPT从语法分析中自动涌现出语义来，是通往AGI的最关键一步。这大概是21世纪以来人类最重要的发现。我看不用说图灵奖，如果你同意AGI是有生命的，也许应该给个「诺贝尔生理学或医学奖」。

✵

暴力破解已经让语言模型如此强大，那么下一步是什么呢？是让它的结构更聪明，以及更重要的是，对AI的驯化。

在跟黄仁勋的对谈里，苏茨科弗讲了OpenAI现在做的事情。GPT已经对世界很了解了，它知道很多，但是需要学会策略性的表达。

一个智者再有学问，问啥说啥也不行。OpenAI不希望GPT什么话都说，他们必须教会它说人们容易接受的话，这意味着比如说政治正确、容易理解、对人更有帮助等等。GPT需要变得更主观一点，最好能以某种符合主流价值观的方式去描述月亮。

而这就不是喂语料的预训练所能解决的了，需要「微调（fine-tune）」和「强化学习」。苏茨科弗说OpenAI使用了两种方法，一个是让人来给它反馈，一个是让另一个AI来训练它。

他说的其实就是现在热门讨论的所谓「AI对齐问题（AI alignment problem）」，也就是让AI的目标和价值观跟主流社会相一致。正如我们前面所讲，现在各个公司都讲价值观，「alignment」已经成了一个流行词汇。

AI研发的风向已经改变。

四月初，OpenAI CEO山姆·奥特曼（Sam Altman）在麻省理工学院做了一个报告，说OpenAI不再追求给GPT增加参数了。这是因为他估计，扩大模型规模的回报已经越来越少。

说白了就是「边际效益递减」。到了这个GPT-4这个规模，你再把模型参数个数增大十倍，你的训练成本，各种支出可能要增加不止十倍，但是模型的表现并不会再好十倍。而且你会受到物理的限制，多少块GPU，多大的数据中心，消耗多少电力，这些都是有上限的。

所以奥特曼说现在的研究方向是改进模型的架构，比如Transformer就还有很大的改进空间。

奥特曼还表示OpenAI并没有在训练GPT-5，现在主要工作还是让GPT-4更有用。这也符合OpenAI之前的暗示：让AI演化得稍微慢一点，让人类能够适应。

所以我们可以预期的是大模型暴力破解阶段已经暂时告一段落。现在的AI已经有了相当程度的智能，近期的首要问题不是让它更聪明，而是更精准、更好用、更能让社会接受。

这一切听起来都很好。但是据我所知，出于某种数学上的原因，OpenAI想要彻底“管住”GPT，是不可能的。咱们下一讲再说。

## 划重点

1.GPT学的其实不是语言，而是语言背后的那个真实世界。GPT这个语言模型，抓住了语言背后的那个东西。
2.我们可以预期的是大模型暴力破解阶段已经暂时告一段落。现在的AI已经有了相当程度的智能，近期的首要问题不是让它更聪明，而是更精准、更好用、更能让社会接受。

![https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689265043906112/051913.jpeg](https://piccdn2.umiwi.com/uploader/image/ddarticle/2023051913/1808689265043906112/051913.jpeg)

---

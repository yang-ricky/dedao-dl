# 第245封信丨AlphaGo的关键方法——强化学习

## 读者，你好！

我们昨天聊了AlphaGo具有超强棋力的硬件条件，今天和你聊聊它在软件上是如何学习下棋的。或者说更广泛地讲，计算机是如何有效地进行学习的。我们先从目前比较热门的一个概念——机器学习（Machine Learning）说起。

机器学习，实际上是寻找一种数学模型，让这种模型符合它所要描述的对象。比如说我们要寻找一种描述天体运动的模型，让它符合太阳系行星的运动情况，今天这个模型就是开普勒－牛顿的椭圆形模型：太阳系中所有的行星和彗星围绕着太阳系的重心（和太阳的位置很接近但是有所差别），做椭圆运动。当然，这个模型内不同物体的运动速度和周期不同，这些数据被称为模型的参数。

有了这样一个模型，就可以知道今后太阳系中行星和彗星的运行规律了，哈雷就用它成功地预测了哈雷彗星未来回到人们视野的时间。在这个模型中，要得到模型的参数（比如每一个星体运动的长轴半径，运动一圈的周期），就要根据观察到的历史数据计算。

开普勒一辈子做的事情大抵如此，他前半辈子跟着老师第谷获得数据，后半辈子做人肉的机器学习——找到模型的参数，让他所预测的行星运动符合所观察到的真实情况。

我们在机器学习中有时用“契合”来形容模型描述的情况和真实情况的接近程度，契合度越高，误差越小，当然也就越好。在数学上有一个“切比雪夫大数定律”，保证了只要数据量足够大，这种契合度就能接近100%。当然，因为开普勒是做“人肉”学习，速度很慢，花了整整半辈子，而今天的计算机做这件事情连万分之一秒都不需要。

机器学习的过程其实和开普勒的工作类似，有了一堆的数据，计算出一个合适的模型。

机器学习根据使用的数据不同区分成了不同的类型。如果使用到的数据是杂乱无章的，机器想从这些数据中学习并得到规律，就被称为无监督的学习，也就是说没有人的监督。

举个例子，假如我们只是把晚上天空的星图拍下来，不标注上每一颗星星的名称，要让计算机自动找出每一颗星星运动的轨迹，这就是无监督的学习。

反过来，如果由人来标识出每一颗星星的名称，再让计算机学习，那就是有监督的学习，也就是说机器学习是在人的监督下进行的。

这两种机器学习的方式各有千秋，无监督的学习很容易获得大量的数据，但是由于没有做人工标识，因此计算机相当于自己在找方向，这样机器学习出来的模型就可能不准确，比如把土星的运动轨迹和木星的相混淆。

有监督的学习则相反，一方面输入的数据比较精确，相当于利用了人类的智力，有了一个大致的方向。但是因为人工标识数据不仅成本高，而且很难得到足够量的数据。这种机器学习方法提高到一定程度，就再也提高不了了。

好了，有了这些背景知识的铺垫，现在可以讲回到AlphaGo的学习方法了。它一开始使用的是有监督的学习，也就是学习那几十万盘人类高手对弈的棋谱。那些对弈输赢的结果是知道的，因此相当于人为地标注上了哪些棋是好棋，哪些棋是差棋。

无论是李世石还是柯洁，一辈子研究的顶级对弈恐怕不过几千盘，也就是AlphaGo的1%，下不过AlphaGo情有可原。但是，人类积累了几千年，一共可查得到的高手对弈也就那么一点点，AlphaGo要是沿着这种思路去学习，改进、进步的速度就太慢了。

另外，和九段棋手学棋，哪怕学得再多，恐怕到10段也就到头了，这样AlphaGo的棋艺就不能比人类高出一大截。

当然，AlphaGo也不能用没有监督的学习，因为围棋的变化太多，虽然计算机速度快，但在围棋领域，试错是试不过来的。

因此，AlphaGo就采用了一个新的学习方法，就是所谓的“强化学习”( Reinforcement Learning)。它的方法其实也很简单，就是计算机在没有人为给定方向的条件下，自己试着走一个方向，然后人告诉它好不好，也就是说有一个反馈信息。

接下来，我就用无人驾驶汽车的学习方法为例来说明这三种机器学习之间的区别。

2006年Google开始做无人驾驶之前，先开展了街景地图项目，不仅获取了全部的道路和街景数据，而且获得了所有时段主要道路的交通状况数据，然后Google对这些数据作了处理，并且和GPS的数据对照做了标识，个别数据还是手工处理的。

因此，无人驾驶汽车的很多模型是利用这些标识好的数据事先进行了有监督的学习。当然，Google无人驾驶汽车上路后，还在不断地进行数据采集，以便让模型越来越准确。由于Google 无人驾驶汽车上路之前的起点和终点是设置好的，目标是明确的，因此它进行的还是有监督的学习。

等到特斯拉搞无人驾驶汽车时，情况就不同了，它事先没有街景、地图等标识好的数据，那么它是怎么做的呢？它就根据人的反馈不断进行学习。比如说它的辅助驾驶功能，一开始基本上只能靠着车道识别的功能，沿着划定得非常清晰的车道行驶。但是到了车道不是很清楚的地方，或者分叉路时，特斯拉就傻眼了，它有一半的可能性要开错车道。

不过这时人会给它一个反馈信息：如果开得对，人就不干预，如果开错了，人会马上手动控制。这样虽然没有标识告诉它怎么开是对的，但是这种反馈接受多了，它将会朝着更好的方向进化。这就是强化学习。

因此，可以认为强化学习虽然没有事先人为标定的数据作参考，但是对于它学习的行为会有反馈意见作指导，如果走错了路，可以纠正过来。

现在我们再次回到AlphaGo的问题上来，如果它只使用人为标注的数据，即人下棋的棋谱，不仅数据有限，而且就算训练出来的模型和人的数据的契合度达到100%，也就是比人的水平高那么一点，发挥更稳定一点，仅此而已，因为人没有下过的棋它学不到。

当然，让它没有目的地试错也不现实。因此Google采用了强化学习的办法，在它作尝试的时候，给它一点反馈，告诉它这个方向对还是不对，这样它不至于胡思乱想。当然，这个反馈怎么得来，则是一门艺术，因为对于人类没有下过的棋，如何判断好坏呢？

对此，Google的AlphaGo团队做了一个所谓的“代理”（Agent），它是一个程序，能够判断棋盘上大致的胜率，每走出若干步之后，如果这个代理认为对于胜率有所提升，那么就说明这个方向是对的，在机器学习上就予以鼓励，否则就予以处罚。最近AlphaGo的团队一直在强调它的算法有改进了，其实就是指这个代理给出反馈的方向更准确了。当然，里面的细节Google从来不公开。

时间长了，AlphaGo的模型就被训练得往胜率增加的方向改进。根据我和Google内部同事了解的一些细节，AlphaGo的训练程序其实很难判断单一一步的胜率变化，但是能够判断若干步累计下来的整体变化，然后它再反过来给每一步走法一个反馈。通过这种方式，AlphaGo就在尝试很多人之前没有尝试过的做法，才实现了超越人类。

AlphaGo的强化学习，其实从原理上讲和我们日常很多做事的方法是一致的。比如教育小孩时，他如果最近表现比较好，就给他一些鼓励，久而久之小孩就被训练得往好的方向发展。

在单位里，上级夸奖下级其实很重要，当下级有些进步时，对他的表扬其实就是对他的一种强化学习训练。类似地，如果他做得不合适，也需要指出，以便改进后往更好的方向发展。对于上级也是如此，如果上级给了下级什么机会，下级表示出感激之情，上级也被强化训练了，因此今后会更照顾这个下级。

我有时在想，人之间的道理和物之间的道理很多时候是相通的。中国人喜欢讲天道，可能这些规律就是一种天道吧！

![https://piccdn3.umiwi.com/img/201706/12/201706121030466131562916.png](https://piccdn3.umiwi.com/img/201706/12/201706121030466131562916.png)

> 吴军
> 
> 你最希望用AlphaGo的技术解决什么现实生活的问题？

 **推荐阅读：**  第124封信丨拉里·佩奇的智慧1：牙刷和爆款、第172封信丨上帝垂青主动的人

---

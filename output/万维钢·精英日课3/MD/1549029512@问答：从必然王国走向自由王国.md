# 问答：从必然王国走向自由王国

## 来自日课：学习一个“深度学习”算法（1）

> 读者 清风明月：反复学习是刺激神经元，相当于不断加大所谓权重的过程。那么权重大小是否在大脑中是神经元之间连接的稳固与否？

> 万维钢

不是加大权重的*大小*，而是加大权重的*确定程度*。一开始神经元给这个输入数据的权重是0.9，但这是一个随机的分配，有很大的不确定性。随着训练的加深，神经网络越来越相信这个权重应该是0.11，参数稳定在这里。看数值，减小了。但是确定性大大增加了。

对比到人，这就好比篮球，训练的目的不是让投篮的用力越来越大，而是越来越准确。

这的确相当于大脑神经元之间的连接越来越稳固！有句话叫 fire together, wire together —— 经常在一起激发的两个神经元会“长”在一起，它们之间的电信号会更强。但是请注意，电信号强并不对应参数权重的数值大，而是对应参数更确定。这就好像发电报汇款，我收到一个很强很强的汇款信号，但这只是一笔很小的钱 —— 信号强烈只是确保钱数不会错。

## 来自日课：学习一个“深度学习”算法（2）

> 读者 熵·维·层：卓老板曾经讲过一个计算机遗传算法：吃豆人游戏。大概方法就是随机的给每个吃豆人移动策略，通过大量的游戏比赛，筛选出较优的策略，然后向下一代遗传，同时又随机改变一些策略，然后再进行大量的游戏比赛，筛选出较优的策略。通过这个方法不停的遗传变化下去。最终得到了非常理想的吃豆策略。我觉得这个吃豆游戏的遗传算法和神经网络算法有非常相似的地方。一是都需要大量的训练，通过大量的数据喂养，来寻找较优方案；二是都在不断的随机调整参数或策略，不好的参数或策略最终被抛弃，好的参数或策略被保留并遗传下去；三是它们找到的参数或策略都不是最优解，不能保证100％正确，但解决具体问题的时候都非常有效。遗传和神经在计算机上如此相似有效，是巧合？还是必然？

> 万维钢

这是非常好的观察。卓老板说的这个遗传算法，和我们这次说的神经网络计算，在人工智能这个领域来说，是极其不同的两个项目，大约可以说是篮球和足球的区别。遗传算法并不需要人工的数据喂养，它自己就能通过输赢来判断优劣，通过一代一代的迭代寻找优秀基因。

二者调整参数的方法也不一样：神经网络每次是主动地调整参数，都有一个方向，就好像投篮一样。这次没进看看是高了还是低了，高了下次瞄准就往低调一调。

而遗传算法则具有更像黑箱的特点。我不知道哪个参数应该往哪个方向调，我甚至都不在乎哪个参数对应的是哪个输入或者输出值。我弄一批有各种参数的算法让他们互相竞争，每次选拔表现最优秀的进行繁殖，中途再加入随机变异。这是模拟生物进化的方法。

因为这个原因，遗传算法选拔出来的方案常常能超出人的想象力。人绝对没想到这个问题还能这么解决，但是它就这么解决了。有时候遗传算法设计一个天线特别好使，可是人类工程师不能解释它为什么这么好使。而对比之下，因为深度学习完全是基于经验的，就不可能拥有比经验更强的能力。

AlphaGo 是深度学习，是通过阅读大量人类棋手的棋谱学习经验。而 AlphaGo Zero 是从零开始自己跟自己对弈的学习，完全不需要学习人类的经验，结果发现了更多在人类棋手看来匪夷所思的下法。我听说 AlphaGo Zero 包含生物进化的思路，但我理解好像不是通常的遗传算法。

但是遗传算法也好，AlphaGo Zero 也好，它们都只适合特定的问题。它们要求这个问题必须是根据一个判断标准进行优化。比如说打游戏下棋的目标就是赢，简单明了，随时可以判断。而图像识别并不具备这个特点，你要识别各种各样不同的物体，每个物体还有不同的形态，你没法明确规定到底什么叫“像”。

这些都是不同点。那为什么这两个项目有你说的那三个相似点呢？我认为这是因为它们解决的都是“有限的数学问题”。生活中的问题千姿百态，我们总是必须先把它变成有限的数学问题，才能交给人工智能处理。所谓有限的数学问题，就是结构固定、参数可调、不要求完美再现真实世界。像围棋这样的项目本身就已经是有限数学问题，天生适合人工智能。但是对于图像识别来说，因为精度总是有限的，就可能有一定的问题。

> 读者 挥剑决浮云：万老师，将这次的“深度学习”，同“刻意练习”对比，我看到它们都是有一个“核心”，通过反馈的信息，让人判断与修正偏差，理出一条向“核心”靠近的路径。不同点：深度学习只有目标，没有具体的套路。刻意练习有目标，有成熟的套路，还有训练过程中完成度的评价体系。追求对套路的临摹程度，然后获得技能。我这样理解是否合适？深度学习，对于个人学习技能有什么帮助？

> 万维钢

把深度学习、乃至一般的机器学习和人的“刻意练习”对比，最大的区别的确在于有没有“套路”。以我之见，套路是人脑思维的长处，但也是局限性。

套路是对经验的抽象总结。下围棋讲“定式”和“手筋”，写文章说要“凤头、猪肚、豹尾”，拍电影说要怎么安排悬念和意外，这些规律我们一听都明白，但是都很难*数学化*。到底什么样的开头算“凤头”呢？难道是字数决定的吗？

人类掌握这些套路，是特别高级的模式识别，很大程度上是“意会”。人不但能理解掌握，而且能举一反三，甚至把套路用在一个完全不同的领域上。比如围棋上有所谓“大场”和“急所”，这两个概念完全可以用在商业竞争方面。AI 没有这些能力。

但是我看 AlphaGo 给我们最大的教训就是，套路也是思维的局限性。套路是特定的视角，是有色的眼镜，是历史的包袱，是经验的枷锁。人类棋手反应 AlphaGo 有些匪夷所思的走法，不符合任何流派的套路。这可能是因为人类的围棋文化还不够成熟，我们还没有总结出那些套路来 —— 事实上你的确能跟着 AI 学到一些新的套路。但也可能是因为围棋这个东西最终是不应该有套路的。如果每时每刻棋盘上都存在着最优的一手，说套路有什么意义呢？

简单地说，这就是“乱拳打倒老师傅”、“无招胜有招”。可是生而为人，不学套路行吗？写诗的最高境界固然是“工夫在诗外”，但当初陆游的原话，最开始说的可是“我初学诗日，但欲工藻绘”！

那这个对比对我们有啥启发呢？ *你得学习套路，但是你不能被套路限制。老师傅的经验不一定是最优解，也许你可以创造新的套路……正所谓“从必然王国走向自由王国”。*

## 来自日课：学习一个“深度学习”算法（3）

> 读者 尹逸：我是一个机器学习专业的博士研究生。我想问万老师关注是否过最近alpha star战胜人类星际争霸玩家，以及像alpha go等系统应用的技术“强化学习”(reinforcement learning)。这一技术并非从大量的训练数据中模仿相应的模式，而是通过试错和反馈来增强系统功能。我认为，数学考试前背例题的类比，不适用于增强学习的系统。我想问万老师，在考虑了强化学习系统以后，您是否仍然认为机器不具有智能呢？

> 万维钢

我认为“智能”的意思是会随机应变，能处理不同的问题。“智能”的反义词是机械化的重复劳动。也许AI界定义的智能不同于“意识”，意识是有就有没有就没有黑白分明的东西，而智能是一个分高地上下连续变化的东西。

“强化学习”不靠背题，随机应变的能力更强，的确比图形识别算法有更强的智能。但是我理解，强化学习仍然是基于经验的。它自己跟自己玩，通过试错发现最优的操作，但这个判断仍然是从经验中得来。它认为这个方法最好，只不过是因为这个方法在过去表现得最好 —— 它不知道*为什么*这个方法表现这么好。

正如朱迪亚·珀尔在《为什么》这本书里所说，知道“为什么”很重要。理解因果关系，下次局面发生剧烈变化的时候，它才知道怎么随机应变。

比如说，强化学习 AI 在《星际争霸》中表现好，那我给它换个游戏打，它能立即上手吗？它得重新训练。它能对游戏提出什么改进意见吗？它能说说游戏里哪个规则对防守的影响最大吗？人类玩家有发展经济、堆兵、突袭这些概念，可能这些概念限制了人类的想象力，但是人类能举一反三。

最简单的测试，就是如果人类棋手跟 Alpha Go 比赛的时候突然要求改变一下规则，比如把原来 19×19 棋盘改成 17×19，看看AI怎么应对。人类棋手会发现以前学的围棋知识几乎都仍然适用，而AI可能不得不重新训练。

![https://piccdn3.umiwi.com/img/201902/01/201902012303111768251444.jpg](https://piccdn3.umiwi.com/img/201902/01/201902012303111768251444.jpg)

---

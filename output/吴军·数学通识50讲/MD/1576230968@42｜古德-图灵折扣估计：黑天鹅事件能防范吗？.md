# 42｜古德-图灵折扣估计：黑天鹅事件能防范吗？

你好，欢迎来到我的《数学通识50讲》，这讲我们讨论黑天鹅事件是否能防范。

## “黑天鹅”是怎么产生的？

 **我们先来看看所谓黑天鹅事件是怎么产生的。**

我们前面讲了通过对文本进行统计，实现通过上下文来预测句子中某个位置单词的例子。在那个例子中，被统计的数据量哪怕再大，相比要产生的条件概率的数量来讲，也显得太小，这种特点我们称之为数据的稀疏性或者干脆称之为数据量不足。

这种情况即使在今天大数据的时代，依然非常严重。就以腾讯为例，虽然它微信上的数据看似很多，但是如果你真要了解某个人在某个时间段的想法，是无法通过直接的统计做到的。因为某个人在一个时间段可能只发送了几百条微信（这已经很多了），从几百条微信的几千个字里，得不到什么可靠的统计信息。

我们根据前面的经验知道，通常要获得95%以上置信度的统计结果，需要被统计的对象出现上千次，但是如果整个样本只有几千字，被统计的对象能出现几次就不错了。这样得到的数据可能和真实的概率相差很远。

造成这个结果的原因是，当我们要进行多个维度的分析时，几个维度变量的组合可能会有太多的情况，以至于每一种组合摊下来的数据都很小。

那么怎么解决这个问题呢？ **我们有两个方法。先讲第一个，古德-图灵折扣估计法，它主要是解决一种被称为零概率的事件。**

我们还是先来看看计算上下文相关性时遇到的零概率问题。我们在前面讲到，“中药”这个词后面跟着“田七”这个词的可能性比较大，如果我们统计足够多的文本，可能会看到成百上千次，于是可以准确估计它的条件概率。

但是“中药”后面跟着“天气”的可能性很小，可能我们统计了10亿词的文本，一次也没有见过，那么我们是否敢说在“中药”条件下，“天气”的条件概率就是零呢？我们并不敢说。过去，人们以为大家讲出来的话，写出来的文字，都符合一些规范，因此有些词的组合就不应该存在。

但是后来人们发现，语言的演变，恰恰是靠将过去认为不可能的组合使用起来所驱动的，比如说今天的很多网络用语过去根本不存在。当它第一次出现时，你不应该将它的概率设置为零。

事实上，如果你把一个10亿词的语料库一分为二，变成A和B两个子集，就会发现很多在A子集中出现的词的前后组合，在B中根本没有遇到，反之亦然。因此我们不能说这种小概率事件的概率等于零。事实上， *将小概率事件的概率强制设定为零，结果就是早晚会遇到黑天鹅事件。*

这就是黑天鹅事件产生的原因，总的来说，就是我们把那些小概率事件，默认为是零概率事件。

## 应对办法一：古德-图灵折扣估计法

那么我们该如何考虑到这些不容易被想到的事件呢？也就是说，如果一个随机事件，我们在统计中没有见到，该如何设置它的概率呢？我们先要来分析一下小概率事件的特点。

我们大家都听说过一个80/20定律， **就是说80%的总量常常是由20%高频率的元素构成的。反过来，80%低频率的元素，或者说长尾的元素，只构成20%的总量。** 这个规律，其实是齐普夫定律（Zipf’s Law）的一个特例。

齐普夫（George Kingsley Zipf）是美国20世纪初的语言学家，他经过对各种语言中词频的统计发现，一个词的排位，和它词频的乘积，近乎是一个常数。

比如在汉语中，“的”是最常见的字，排位第一，它的字频大约是6%，于是1x6%=6%。第二高频字是“是”这个字，排位第二，而它的字频大约是3%，恰好2x3%=6%。字频排位第三的字是“一”，它的字频是2%多一点，3x2%也是6%。

后来经济学家和社会学家发现齐普夫定律在他们的学科中也成立，比如你如果把世界上每一个人的财富排一个序，让序号乘以财富的数量，就会发现有类似的规律。 *今天，齐普夫定律被认为是自然界的普遍规律。我们每一个人都需要牢记齐普夫定律，这样就不会相信所有人都能够通过创业成为富翁这样的鸡汤观点了，因为它违背齐普夫定律。*

不仅如此，齐普夫定律在低频词上也有一个出乎意料的特点，就是词频乘以那个频率的词的数量，也近乎是一个常数。比如在一个词汇表中，大量的词只出现一次，但是这些词的总数甚至占到了词汇表的一半左右，然后还有大量的出现两三次的词，总数也不少。

如果我们假定只出现一次的词有N1个，出现两次的词有N2个，出现三次的词有N3个，那么1xN1，和2xN2，3xN3，都差不太多，因为大多数词其实只出现一次。

图灵的一个学生古德，他就想利用这个特点，设计一个办法，把那些黑天鹅事件也考虑到。于是设计了一种对低频随机事件，特别是统计中没有见过的随机事件分配概率的方法。

古德的想法是这样的。假如出现r次的单词有Nr个，那么一个语料库文本中所有单词的总次数就是：

 **C=1xN1+2xN2+3xN3+…..+KxNK，** 其中K是最高的词频。

当然在这个统计中，更重要的是考虑那些原本没被考虑进来的词，这些词在之前的统计中出现0次，其实根本没有被统计进来，我们现在假定这些单词有N0个。这并非代表这些词的频率就该是零，而是统计量不够多，要知道统计量和统计量之外的总量还是有区别的。

古德根据经验，假设N0>N1，也就是说假设那些没被统计进来的词，也可能出现了至少一次，这个假设在计算语言学中是符合实际情况的。

接下来古德就调整不同词的词频。他是这么做的，一个单词如果原来出现了0次，他就把出现的次数调整为 N1/N0次。注意，这是一个0到1之间的数，不再是0了。一个单词如果原来出现了1次，他就把出现的次数调整为 2xN2/N1次，通常这是一个1到2之间的数。

对于一般情况，如果原来出现了r次，就调整为（r+1）xNr+1/Nr次。

古德的做法实际上就是把出现1次的单词的总量，给了出现0次的，出现2次单词的总量给了出现1次的，以此类推。这样，对于低频词，它的频率会被提高一点，对于高频词，会降低一点。用我们过去在微积分中的术语来讲，这样让所有词的概率分布变得更光滑了。

古德的这种做法被称为“古德-图灵折扣估计”，因为它实际上是把高频词的词频打了一个折，多出来的词频分配给了低频词。后来人们对这种分配频率的方法作了一些改进，但是原理和古德的做法大同小异。今天在各种统计中，要想避免零概率事件带来的灾难，都会对统计结果作一次类似的处理。

## 应对办法二：插值法

古德的这种方法虽然解决了零概率的问题，但是依然没有解决数据稀疏时，对于小概率事件，概率估计不准确的问题。

 **最初解决这个问题的是我的导师贾里尼克，他发明了一种被称为“删除插值”的方法，我们简称“插值法”。**

贾里尼克在对文本统计时发现，当我们在计算条件概率时加入太多的条件，统计的结果就非常稀疏不可靠。这也很好理解，如果我们不考虑任何条件，只统计词频，每个词X其实会出现很多次，统计的结果就比较可靠。

但是，如果我们考虑了上下文信息，比如前面的一个词，我们要计算条件概率P(X|Y)，我们就要按照X和Y所有可能的组合进行统计，由于组合的数量巨大，因此每一种情况看到的次数就少很多。对于频率较低的（X，Y）组合，计算出的概率P(X|Y)肯定不准确。那么怎么办呢？

贾里尼克讲，我把条件概率和非条件概率加起来，得到一个新的概率。当然在相加之前，要分别给这两个概率权重。比如条件概率的权重是0.7，非条件概率的权重是0.3。

这样一来，如果条件P(X|Y)本身比较大，它在新的概率估计中会占主导地位。如果P(X|Y)本身比较小，说明它反正也不太可靠，而这时非条件概率，即P(X)本身则占了主导地位，因为X本身出现的次数会比较多，统计结果可信度会高一些。比如计算“中药”条件下“天气”这个词的概率时，统计的结果就不可靠，我们就降低它的权重，然后用“天气”本身的概率去弥补这一点不足。

 *插值法的精髓在于，相信那些见到次数比较多的统计结果，如果遇到统计数量不足时，就设法找一个可靠的统计结果来近似。* 后来又有人进一步改进了插值法，提出了备用法（Back-off）。这里面的细节大家就不用关注了，理解它解决不可靠统计问题的精髓就好。

顺便说一句， **在数学上和信息论上可以证明，无论是插值法还是备用法，都比单纯依靠统计结果直接产生概率模型更准确。**

 **要点总结：**

我们说防范黑天鹅事件，不是停留在嘴上的，数学家们会发明一些方法，让得到的数学模型能够预防黑天鹅事件的发生。比如古德是通过将高频事件的概率分配给低频事件，而贾里尼克则是设计一种组合，让那些更可靠的统计发挥更大的作用，同时降低不可靠的统计结果的影响力。

这两点无论是在科学研究还是在生活中，都是很有效的方法论。 *很多时候，我们需要像古德那样预防不测，当然凡事都有成本，预防不测的成本在于要降低一些高收益项目的利润。同时，我们也要像贾里尼克那样，永远更多地依赖可靠的统计结果。*

到此为止，我们关于概率论和统计的内容就基本上讲完了，在接下来的两讲，我们讲一下从概率论延伸出来的课题——博弈论。我们下一讲再见。

![https://piccdn3.umiwi.com/img/201912/13/201912131811301323176821.jpg](https://piccdn3.umiwi.com/img/201912/13/201912131811301323176821.jpg)

---
